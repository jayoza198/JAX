{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU",
    "gpuClass": "standard"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import jax.numpy as jnp\n",
        "from jax import grad, jit, vmap\n",
        "from jax import random"
      ],
      "metadata": {
        "id": "LTcpt4P6p-Ol"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# A helper function to randomly initialize weights and biases\n",
        "# for a dense neural network layer\n",
        "def random_layer_params(m, n, key, scale=1e-2):\n",
        "  w_key, b_key = random.split(key)\n",
        "  return scale * random.normal(w_key, (n, m)), scale * random.normal(b_key, (n,))\n",
        "\n",
        "# Initialize all layers for a fully-connected neural network with sizes \"sizes\"\n",
        "def init_network_params(sizes, key):\n",
        "  keys = random.split(key, len(sizes))\n",
        "  return [random_layer_params(m, n, k) for m, n, k in zip(sizes[:-1], sizes[1:], keys)]\n",
        "\n",
        "layer_sizes = [784, 512, 512, 10]\n",
        "step_size = 0.01\n",
        "num_epochs = 10\n",
        "batch_size = 128\n",
        "n_targets = 10\n",
        "params = init_network_params(layer_sizes, random.PRNGKey(0))"
      ],
      "metadata": {
        "id": "l5_IfrHoq3rF"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from jax.scipy.special import logsumexp\n",
        "\n",
        "def relu(x):\n",
        "  return jnp.maximum(0, x)\n",
        "\n",
        "def predict(params, image):\n",
        "  # per-example predictions\n",
        "  activations = image\n",
        "  for w, b in params[:-1]:\n",
        "    outputs = jnp.dot(w, activations) + b\n",
        "    activations = relu(outputs)\n",
        "  \n",
        "  final_w, final_b = params[-1]\n",
        "  logits = jnp.dot(final_w, activations) + final_b\n",
        "  return logits - logsumexp(logits)"
      ],
      "metadata": {
        "id": "ZJ8Blitgq8Qc"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This works on single examples\n",
        "random_flattened_image = random.normal(random.PRNGKey(1), (28 * 28,))\n",
        "preds = predict(params, random_flattened_image)\n",
        "print(preds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "trSMDElhq_e0",
        "outputId": "f6150a9e-a1b2-4963-906a-e0b5929c7bde"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10,)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Doesn't work with a batch\n",
        "random_flattened_images = random.normal(random.PRNGKey(1), (10, 28 * 28))\n",
        "try:\n",
        "  preds = predict(params, random_flattened_images)\n",
        "except TypeError:\n",
        "  print('Invalid shapes!')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tTrQowMNrB5M",
        "outputId": "6846a083-326c-4fdd-c948-b4cfba523d6f"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Invalid shapes!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Let's upgrade it to handle batches using `vmap`\n",
        "\n",
        "# Make a batched version of the `predict` function\n",
        "batched_predict = vmap(predict, in_axes=(None, 0))\n",
        "\n",
        "# `batched_predict` has the same call signature as `predict`\n",
        "batched_preds = batched_predict(params, random_flattened_images)\n",
        "print(batched_preds.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "VO-Re5uMrKKB",
        "outputId": "ad756f9f-da36-4c52-e544-9b02361fd31f"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "(10, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def one_hot(x, k, dtype=jnp.float32):\n",
        "  \"\"\"Create a one-hot encoding of x of size k.\"\"\"\n",
        "  return jnp.array(x[:, None] == jnp.arange(k), dtype)\n",
        "  \n",
        "def accuracy(params, images, targets):\n",
        "  target_class = jnp.argmax(targets, axis=1)\n",
        "  predicted_class = jnp.argmax(batched_predict(params, images), axis=1)\n",
        "  return jnp.mean(predicted_class == target_class)\n",
        "\n",
        "def loss(params, images, targets):\n",
        "  preds = batched_predict(params, images)\n",
        "  return -jnp.mean(preds * targets)\n",
        "\n",
        "@jit\n",
        "def update(params, x, y):\n",
        "  grads = grad(loss)(params, x, y)\n",
        "  return [(w - step_size * dw, b - step_size * db)\n",
        "          for (w, b), (dw, db) in zip(params, grads)]"
      ],
      "metadata": {
        "id": "3WCLvDmTqGIh"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import tensorflow as tf\n",
        "# Ensure TF does not see GPU and grab all GPU memory.\n",
        "tf.config.set_visible_devices([], device_type='GPU')\n",
        "\n",
        "import tensorflow_datasets as tfds\n",
        "\n",
        "data_dir = '/tmp/tfds'\n",
        "\n",
        "# Fetch full datasets for evaluation\n",
        "# tfds.load returns tf.Tensors (or tf.data.Datasets if batch_size != -1)\n",
        "# You can convert them to NumPy arrays (or iterables of NumPy arrays) with tfds.dataset_as_numpy\n",
        "mnist_data, info = tfds.load(name=\"mnist\", batch_size=-1, data_dir=data_dir, with_info=True)\n",
        "mnist_data = tfds.as_numpy(mnist_data)\n",
        "train_data, test_data = mnist_data['train'], mnist_data['test']\n",
        "num_labels = info.features['label'].num_classes\n",
        "h, w, c = info.features['image'].shape\n",
        "num_pixels = h * w * c\n",
        "\n",
        "# Full train set\n",
        "train_images, train_labels = train_data['image'], train_data['label']\n",
        "train_images = jnp.reshape(train_images, (len(train_images), num_pixels))\n",
        "train_labels = one_hot(train_labels, num_labels)\n",
        "\n",
        "# Full test set\n",
        "test_images, test_labels = test_data['image'], test_data['label']\n",
        "test_images = jnp.reshape(test_images, (len(test_images), num_pixels))\n",
        "test_labels = one_hot(test_labels, num_labels)"
      ],
      "metadata": {
        "id": "ZPtaq44vptuo"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print('Train:', train_images.shape, train_labels.shape)\n",
        "print('Test:', test_images.shape, test_labels.shape)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "z_oVg7wYp2Ry",
        "outputId": "8db905a0-691d-462a-e81e-6b4d2f4607d2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Train: (60000, 784) (60000, 10)\n",
            "Test: (10000, 784) (10000, 10)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import time\n",
        "\n",
        "def get_train_batches():\n",
        "  # as_supervised=True gives us the (image, label) as a tuple instead of a dict\n",
        "  ds = tfds.load(name='mnist', split='train', as_supervised=True, data_dir=data_dir)\n",
        "  # You can build up an arbitrary tf.data input pipeline\n",
        "  ds = ds.batch(batch_size).prefetch(1)\n",
        "  # tfds.dataset_as_numpy converts the tf.data.Dataset into an iterable of NumPy arrays\n",
        "  return tfds.as_numpy(ds)\n",
        "\n",
        "for epoch in range(num_epochs):\n",
        "  start_time = time.time()\n",
        "  for x, y in get_train_batches():\n",
        "    x = jnp.reshape(x, (len(x), num_pixels))\n",
        "    y = one_hot(y, num_labels)\n",
        "    params = update(params, x, y)\n",
        "  epoch_time = time.time() - start_time\n",
        "\n",
        "  train_acc = accuracy(params, train_images, train_labels)\n",
        "  test_acc = accuracy(params, test_images, test_labels)\n",
        "  print(\"Epoch {} in {:0.2f} sec\".format(epoch, epoch_time))\n",
        "  print(\"Training set accuracy {}\".format(train_acc))\n",
        "  print(\"Test set accuracy {}\".format(test_acc))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "U2XApBFPqHMo",
        "outputId": "0061d9b5-5d96-4a01-9799-730f7ed2999d"
      },
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0 in 9.48 sec\n",
            "Training set accuracy 0.9253000020980835\n",
            "Test set accuracy 0.9269999861717224\n",
            "Epoch 1 in 5.07 sec\n",
            "Training set accuracy 0.9428666830062866\n",
            "Test set accuracy 0.9411999583244324\n",
            "Epoch 2 in 4.30 sec\n",
            "Training set accuracy 0.9531999826431274\n",
            "Test set accuracy 0.9514999985694885\n",
            "Epoch 3 in 4.44 sec\n",
            "Training set accuracy 0.9600000381469727\n",
            "Test set accuracy 0.9555999636650085\n",
            "Epoch 4 in 5.52 sec\n",
            "Training set accuracy 0.9651666879653931\n",
            "Test set accuracy 0.9605000019073486\n",
            "Epoch 5 in 4.56 sec\n",
            "Training set accuracy 0.9691833257675171\n",
            "Test set accuracy 0.9632999897003174\n",
            "Epoch 6 in 5.06 sec\n",
            "Training set accuracy 0.9724833369255066\n",
            "Test set accuracy 0.9655999541282654\n",
            "Epoch 7 in 4.33 sec\n",
            "Training set accuracy 0.975516676902771\n",
            "Test set accuracy 0.9666999578475952\n",
            "Epoch 8 in 4.26 sec\n",
            "Training set accuracy 0.9779833555221558\n",
            "Test set accuracy 0.9680999517440796\n",
            "Epoch 9 in 5.07 sec\n",
            "Training set accuracy 0.9802666902542114\n",
            "Test set accuracy 0.9688999652862549\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Qa8jPJkdqsbk"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}